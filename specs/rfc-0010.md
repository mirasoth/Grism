# RFC-0010: Distributed & Parallel Execution

**Status**: Draft
**Authors**: Grism Team
**Created**: 2026-01-21
**Last Updated**: 2026-01-23
**Depends on**: RFC-0007, RFC-0008, RFC-0009
**Supersedes**: —

---

## 1. Abstract

This RFC defines the **distributed and parallel execution model** for Grism.

Distributed execution is treated as a **physical execution concern**, not a logical one. This document specifies:

* What operators may be parallelized
* How data may be partitioned
* What semantic guarantees MUST be preserved
* How failures, skew, and coordination are handled

This RFC ensures that scaling out **never changes meaning**.

---

## 2. Scope and Non-Goals

### 2.1 Scope

This RFC specifies:

* Intra-node and inter-node parallelism
* Data partitioning and sharding
* Distributed operator constraints
* Coordination and synchronization points
* Failure and retry semantics

### 2.2 Non-Goals

This RFC does **not** define:

* Cluster management or scheduling
* Networking protocols
* Elastic scaling policies
* Cloud provider integration
* Query admission control

---

## 3. Design Principles

1. **Semantics Are Sacred**
   Distribution MUST NOT change results.

2. **Operator-Driven Parallelism**
   Parallelism is enabled by operator capabilities.

3. **Partition-Aware Execution**
   Data locality matters more than raw parallelism.

4. **Failure Is a First-Class Case**
   Partial execution MUST be recoverable or abortable.

---

## 4. Parallelism Model

### 4.1 Levels of Parallelism

Grism supports:

| Level    | Description                                |
| -------- | ------------------------------------------ |
| Operator | Independent operators execute concurrently |
| Pipeline | Streaming operators run in parallel        |
| Data     | Input partitions processed concurrently    |
| Cluster  | Execution spans multiple nodes             |

---

### 4.2 Determinism & Parallelism

Parallel execution MUST preserve:

* Row-level semantics
* Aggregate correctness
* Three-valued logic
* Deterministic expression behavior (RFC-0003)

Row ordering is **not guaranteed** unless explicitly requested.

---

## 5. Data Partitioning

### 5.1 Partitioning Strategies

Supported strategies:

* Hash partitioning
* Range partitioning
* Label / role-based partitioning
* Adjacency-based partitioning

Partitioning strategy MUST be explicit in physical plans.

---

### 5.2 Partitioning Invariants

Partitions MUST:

* Cover the entire input (unless filtered)
* Be disjoint
* Preserve schema consistency

---

## 6. Operator Parallelizability

### 6.1 Parallel-Safe Operators

The following are **inherently parallelizable**:

* Scan
* Filter
* Project
* Expand (with adjacency locality)
* Limit (with coordination)

---

### 6.2 Conditionally Parallel Operators

| Operator  | Condition                   |
| --------- | --------------------------- |
| Union     | Inputs co-partitioned       |
| Aggregate | Partial aggregation allowed |
| Sort      | Range partitioning          |

**Important**: There is no Join operator. Relational composition is handled via Expand operators.

---

### 6.3 Blocking Operators

Blocking operators introduce synchronization:

* Global Aggregate
* Global Sort
* Collect / Materialize

Blocking operators MUST declare barriers.

---

## 7. Distributed Expand Semantics

Expand is **distribution-sensitive**.

Rules:

* Prefer adjacency-local execution
* Cross-partition expansion MUST be explicit
* Hyperedge expansion may require coordination

Adjacency locality is a **hard preference**, not an optimization hint.

---

## 8. Shuffle & Exchange Operators

### 8.1 Exchange Operator

`Exchange` is an explicit physical operator:

* Repartitions data
* Introduces network shuffle
* Acts as a barrier

Exchange MUST be explicit in physical plans.

---

### 8.2 Exchange Guarantees

* Data completeness
* Partitioning correctness
* Backpressure support

---

## 9. Fault Tolerance & Recovery

### 9.1 Failure Model

Failures may include:

* Worker crash
* Network partition
* Data source failure

---

### 9.2 Recovery Rules

* Stateless operators may be retried
* Stateful operators require checkpointing
* Blocking operators require restart or recovery

Exactly-once semantics are **not guaranteed** by default.

---

## 10. Consistency & Isolation

Distributed execution assumes:

* Snapshot isolation for reads
* No mid-query mutation visibility
* Consistent index views

Transactional semantics are out of scope.

---

## 11. Execution Coordination

### 11.1 Control Plane Responsibilities

* Plan dissemination
* Operator placement
* Progress tracking
* Cancellation propagation

---

### 11.2 Data Plane Responsibilities

* Batch transport
* Backpressure
* Flow control

Control and data planes MUST be decoupled.

---

## 12. Distributed Runtime

This section defines semantic requirements for distributed execution. For implementation details, see RFC-0102.

### 12.1 Ray Runtime (Primary Distributed Backend)

> **Ray orchestrates, Rust executes.**

Ray handles task scheduling, data movement, and fault tolerance, while Rust workers perform actual query execution using the same operators as local execution.

**Semantic Requirements**:
* Physical operator fragments execute identically to local execution
* Data transport preserves Arrow RecordBatch semantics
* Zero-copy sharing when possible

### 12.2 Workload Characteristics

| Workload Type | Parallelism Strategy |
|---------------|---------------------|
| Relational (filter/project/aggregate) | Data parallelism with shuffle |
| Graph (traversal) | Adjacency-aware partitioning |
| Hybrid | Mixed strategies per subplan |

### 12.3 Partitioning Requirements

* Partitioning by node / hyperedge ID ranges for graph workloads
* Cross-partition Expand requires explicit Exchange operator
* Adjacency locality preserved within partitions

---

## 13. Explainability & Observability

Distributed execution MUST expose:

* Per-stage execution time
* Shuffle volume
* Partition skew
* Retry events

This is mandatory for debugging and trust.

---

## 14. Error Handling & Cancellation

* Cancellation MUST propagate cluster-wide
* Partial results MUST be discarded
* Errors MUST surface deterministically

Best-effort cleanup is required.

---

## 15. Relationship to Other RFCs

* **RFC-0007**: Cost model influences distribution
* **RFC-0008**: Physical operators define capabilities
* **RFC-0009**: Access paths constrain partitioning
* **RFC-0011**: Runtime scheduling and backpressure
* **RFC-0102**: Execution engine architecture (implements this RFC)

RFC-0010 defines **how Grism scales**. RFC-0102 provides the authoritative implementation reference for the Ray distributed runtime.

---

## 16. Open Questions

* Adaptive repartitioning
* Speculative execution
* Learned skew mitigation
* Distributed vector search semantics

---

## 17. Conclusion

This RFC defines **how Grism executes at scale**—without sacrificing correctness.

> **Parallelism accelerates execution.
> Distribution requires careful coordination.
> RFC-0010 ensures only speed is amplified, not errors.**
