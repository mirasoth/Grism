# RFC-0021: Cloud / ObjectStore Lance Backend

**Status**: Draft
**Authors**: Grism Team
**Created**: 2026-01-23
**Last Updated**: 2026-01-23
**Depends on**: RFC-0009, RFC-0012, RFC-0018, RFC-0019, RFC-0102
**Supersedes**: —

---

## 1. Abstract

This RFC defines the **cloud / object-store-based storage backend** for Grism using **Lance datasets over remote object storage** (e.g., S3-compatible systems).

This backend generalizes the Lance-based local storage backend (RFC-0019) to operate over:

* Cloud object stores (S3, GCS, Azure Blob)
* On-premise S3-compatible systems

The RFC specifies how Grism storage abstractions are preserved under remote storage constraints, including latency, consistency, and access granularity.

---

## 2. Scope and Non-Goals

### 2.1 Scope

This RFC specifies:

* Mapping of Grism snapshots and fragments to object-store paths
* Lance dataset usage over object storage
* Capability signaling specific to remote storage
* Interaction boundaries with distributed execution engines

### 2.2 Non-Goals

This RFC does **not** define:

* Distributed execution scheduling
* Multi-writer concurrency or transactions
* Object-store-specific tuning parameters
* Security, IAM, or credential management

---

## 3. Design Principles

### 3.1 Behavioral Parity with Local Lance Backend

The cloud Lance backend MUST preserve the same logical behavior as RFC-0019:

* Identical schemas
* Identical fragment semantics
* Identical snapshot isolation guarantees

Differences are limited to performance characteristics and capabilities.

---

### 3.2 Object Store as a Passive Persistence Layer

Object storage is treated as a **passive byte store**.

Grism MUST NOT rely on:

* Atomic directory operations
* Rename semantics
* Strong consistency guarantees

---

### 3.3 Snapshot Immutability

Each snapshot corresponds to a **write-once, immutable object namespace**.

Mutable operations MUST occur outside the scope of this RFC.

---

## 4. Object Store Layout

Snapshots are mapped to object prefixes:

```
<bucket>/<grism_root>/snapshots/<snapshot_id>/
  ├── nodes/
  │   └── <label>.lance/
  ├── hyperedges/
  │   └── <label>.lance/
  └── adjacency/
      └── <adjacency_spec>.lance/
```

The layout mirrors RFC-0019 exactly, differing only in storage medium.

---

## 5. Lance Dataset Access

### 5.1 Remote Dataset Opening

Lance datasets are opened using object-store-aware URIs:

```
s3://<bucket>/<path>/<dataset>.lance
```

All access MUST be compatible with Lance’s object-store abstraction.

---

### 5.2 Read Patterns

The backend SHOULD optimize for:

* Sequential fragment reads
* Column projection pushdown
* Predicate pushdown when supported

Random access patterns SHOULD be avoided by planners.

---

## 6. Node and Hyperedge Storage

Node and hyperedge datasets follow the same schema and semantics as RFC-0019.

No cloud-specific extensions are permitted at the schema level.

---

## 7. Adjacency Storage

Adjacency datasets are materialized identically to the local Lance backend.

However:

* Fragment sizes SHOULD be larger to amortize network overhead
* Fragment counts SHOULD be minimized

AdjacencyFragmentMeta remains the authoritative interface.

---

## 8. Fragment Semantics

### 8.1 Fragment Addressing

Fragments are addressed by:

* Object path
* Byte ranges
* Lance fragment identifiers

Fragment metadata MUST be sufficient for remote pruning.

---

### 8.2 Fragment Stability

Fragments are immutable once published.

Object-store eventual consistency MUST NOT affect fragment identity.

---

## 9. Storage Capabilities

The cloud Lance backend advertises:

```
StorageCaps {
  predicate_pushdown: true,
  projection_pushdown: true,
  fragment_pruning: true,
  object_store: true,
}
```

Capabilities MAY vary by object store and are discoverable at runtime.

---

## 10. Snapshot Resolution

`Storage::resolve_snapshot()` resolves to an object-store-backed snapshot handle.

No directory listing assumptions are permitted beyond prefix-based enumeration.

---

## 11. Planner and Execution Interaction

Planners MUST:

* Prefer fragment-sequential scans
* Avoid fine-grained random access

Execution engines MAY cache fragments locally.

---

## 12. Failure and Consistency Model

The backend assumes:

* Read-after-write consistency for individual objects
* No transactional guarantees across objects

Snapshot publication MUST be externally coordinated.

---

## 13. Relationship to Other RFCs

* **RFC-0012**: Storage abstractions
* **RFC-0018**: Persistent storage & adjacency layout
* **RFC-0019**: Lance-based local storage backend
* **RFC-0020**: In-memory storage backend
* **RFC-0102**: Execution engine architecture

RFC-0021 defines the **cloud-backed sibling** of RFC-0019.

---

## 14. Summary

The cloud / object-store Lance backend provides:

* A scalable persistent storage layer
* Compatibility with distributed execution engines
* Clear isolation between storage semantics and cloud infrastructure

It completes Grism’s core storage backend matrix.
