# RFC-0012: Storage & Persistence Layer

**(Core Design Principles & Abstract Architecture)**

**Status**: Review
**Authors**: Grism Team
**Created**: 2026-01-21
**Last Updated**: 2026-01-23
**Depends on**: RFC-0002, RFC-0008, RFC-0010, RFC-0100, RFC-0102
**Supersedes**: —

**Scope**: This RFC defines the core design principles and abstract architecture of the Grism engine, with a particular focus on storage abstractions and their interaction with execution runtimes. This document is fully aligned with RFC-0102 and adopts its terminology and execution model as authoritative.

---

## 1. Purpose and Non-Goals

### 1.1 Purpose

RFC-0012 establishes the *conceptual and contractual foundation* of Grism. It defines:

* Core architectural principles
* Abstract storage interfaces
* Snapshot and consistency semantics
* The boundary between storage and execution

This RFC ensures that all execution runtimes (local, Ray-distributed, and future runtimes) interact with storage in a **uniform, deterministic, and runtime-agnostic** manner.

### 1.2 Non-Goals

This RFC does **not** define:

* Physical execution plans or operators (see RFC-0008 for contracts, RFC-0102 for implementation)
* Query languages or APIs
* Distributed scheduling or fault tolerance
* Transactional write semantics

---

## 2. Design Goals

The core design goals of Grism are:

1. **General and Consistent Storage Interface**
   All storage backends must implement a single, unified interface independent of execution runtime.

2. **Execution–Storage Decoupling**
   Storage must remain execution-agnostic and unaware of runtime topology, scheduling, or parallelism.

3. **Snapshot-Based Determinism**
   All reads operate on immutable snapshots, guaranteeing reproducibility across runtimes.

4. **Arrow-Native Data Exchange**
   Storage exposes data exclusively as Arrow `RecordBatch` streams.

5. **Runtime Equivalence**
   Local and distributed execution must observe identical storage semantics.

6. **Execution Context Compatibility**
   Storage access is permitted *only* via the `ExecutionContextTrait` defined in RFC-0102.

---

## 3. Architectural Overview

At the highest level, Grism is structured as three orthogonal layers:

```
┌────────────────────────────┐
│        User Interfaces     │
│   (Python APIs, Agents)    │
└────────────▲───────────────┘
             │
┌────────────┴───────────────┐
│        Execution Layer     │
│ (Physical Plans, Operators)│  ← RFC-0102
└────────────▲───────────────┘
             │ ExecutionContextTrait
┌────────────┴───────────────┐
│         Storage Layer      │  ← RFC-0012
│ (Snapshots, Fragments)     │
└────────────────────────────┘
```

This RFC defines the **Storage Layer** and its abstract contract with the Execution Layer.

---

## 4. Core Design Principles

### 4.1 Storage Is Execution-Agnostic

The storage layer:

* Is accessed exclusively through `ExecutionContextTrait::storage()`
* Does not inspect physical plans, operators, or runtime state
* Does not differentiate between local or distributed execution

Storage MUST NOT:

* Schedule tasks
* Push data into execution
* Observe executor lifecycles

Execution *pulls* data from storage; storage never initiates execution.

---

### 4.2 Execution Context as the Sole Gateway

The `ExecutionContextTrait` (RFC-0102) is the *only* mechanism by which execution interacts with storage.

```text
ExecutionContextTrait
├── storage() → Storage
├── snapshot_id() → SnapshotId
├── memory_manager()
├── metrics_sink()
└── is_cancelled()
```

All physical operators MUST:

* Obtain storage handles from the execution context
* Use the snapshot identifier provided by the execution context

Direct storage access outside an execution context is forbidden.

---

### 4.3 Pull-Based Data Flow

Storage exposes data as **pull-based Arrow `RecordBatch` streams**.

* Execution controls iteration and consumption
* Storage does not control ordering or concurrency
* Backpressure is naturally enforced by the executor

This model guarantees compatibility with both synchronous and distributed runtimes.

---

## 5. Storage Abstractions

### 5.1 Storage Trait

All storage backends MUST implement the following abstract interface:

```rust
trait Storage {
    fn resolve_snapshot(&self, spec: SnapshotSpec) -> SnapshotId;

    fn scan(
        &self,
        dataset: DatasetId,
        projection: &Projection,
        predicate: Option<Predicate>,
        snapshot: SnapshotId,
    ) -> RecordBatchStream;

    fn fragments(
        &self,
        dataset: DatasetId,
        snapshot: SnapshotId,
    ) -> Vec<FragmentMeta>;

    fn capabilities(&self) -> StorageCaps;
}
```

#### Normative Guarantees

* `scan()` returns a pull-based Arrow `RecordBatch` stream
* Fragment boundaries are stable for a given `SnapshotId`
* The interface is runtime-neutral and executor-agnostic

---

### 5.2 Fragment Model

A **Fragment** represents a stable, addressable unit of persisted data.

* Identified by `FragmentMeta`
* Immutable within a snapshot
* Suitable for parallel scanning

Fragments form the bridge between storage layout and execution parallelism, without coupling the two.

For persistent layout specifications (nodes, hyperedges, adjacency fragments), see **RFC-0018**.

---

### 5.3 Storage Capabilities

`StorageCaps` advertises optional backend features such as:

* Predicate pushdown
* Projection pushdown
* Fragment-level pruning
* Object-store compatibility

Execution MAY adapt plans based on capabilities but MUST NOT rely on undocumented behavior.

---

## 6. Snapshot Model

### 6.1 SnapshotId

All reads occur against a `SnapshotId` supplied by:

```text
ExecutionContextTrait::snapshot_id()
```

A `SnapshotId`:

* Represents an immutable view of storage state
* Is consistent across all operators in a single execution
* Is independent of runtime clocks or executor behavior

---

### 6.2 Snapshot Semantics

Storage MUST NOT:

* Implicitly create snapshots
* Mutate snapshot contents
* Depend on execution order

This ensures deterministic and reproducible execution across runtimes.

---

## 7. Storage Backends

### 7.1 Local Runtime Backends

For the local execution engine, the following backends are supported:

| Backend           | Persistence | Description                        |
| ----------------- | ----------- | ---------------------------------- |
| `MemoryStorage`   | None        | Ephemeral, testing and prototyping |
| `LanceStorage`    | Local FS    | Persistent, Lance-based datasets   |

Both conform strictly to the `Storage` trait.

---

### 7.2 Distributed Runtime Backends (Ray)

For Ray-based distributed execution, storage is backed by cloud object stores:

* S3
* GCS
* Azure Blob
* Other cloud object stores

Key requirements:

* Fragment-addressable
* Safe for concurrent access by Ray workers
* No assumptions about local filesystem availability

The same `Storage` interface is used without modification.

---

## 8. Storage and Execution Interaction

### 8.1 Interaction Pattern

The canonical interaction pattern is:

```text
PhysicalOperator
  → ExecutionContextTrait
      → Storage
      → SnapshotId
```

Storage never observes:

* Operator identity
* Execution stages
* Runtime topology

Execution never observes:

* Storage layout internals
* Physical file placement

---

## 9. Runtime Equivalence Guarantee

Given the same:

* Physical plan
* SnapshotId
* Storage backend

Local and Ray execution MUST produce identical logical results.

Any divergence is considered a violation of this RFC.

---

## 10. Relationship to Other RFCs

* **RFC-0102**: Defines execution architecture, physical operators, and `ExecutionContextTrait`. Authoritative for execution semantics.
* **RFC-0012 (this document)**: Authoritative for storage abstractions, snapshot semantics, and persistence boundaries.

Neither RFC may redefine the other’s domain.

---

## 11. Summary

RFC-0012 establishes storage as a **pure, deterministic, execution-agnostic subsystem**. By enforcing strict boundaries and shared abstractions with RFC-0102, it ensures:

* Clean separation of concerns
* Runtime-independent correctness
* Long-term extensibility

This foundation enables Grism to evolve execution strategies without destabilizing storage semantics.
