# RFC-0102: Execution Engine Architecture

**Status**: Review
**Authors**: Grism Team
**Created**: 2026-01-22
**Last Updated**: 2026-01-23
**Depends on**: RFC-0002, RFC-0008, RFC-0010, RFC-0100
**Supersedes**: —

---

## 1. Abstract

This RFC defines the **execution engine architecture** for Grism, specifying how logical plans are transformed into physical plans and executed across different runtime environments.

The engine architecture is structured around two crates with three distinct concerns:

1. **Common Engine Layer** (in grism-engine): Runtime-agnostic physical planning, operators, and expression evaluation
2. **Local Runtime** (in grism-engine): Single-machine execution with pull-based streaming
3. **Ray Runtime** (in grism-ray): Distributed execution with stage-based parallelism

The common engine layer and local runtime are combined in `grism-engine` for simplicity, while distributed execution is isolated in `grism-ray`. This separation ensures that execution semantics remain identical regardless of runtime environment while allowing each runtime to optimize for its specific characteristics.

---

## 2. Scope and Non-Goals

### 2.1 Scope

This RFC specifies:

* Crate organization and responsibilities
* Physical operator model and contracts
* Expression evaluation architecture
* Local execution model
* Distributed execution model
* Runtime abstraction interfaces

### 2.2 Non-Goals

This RFC does **not** define:

* Storage engine internals (see RFC-0012)
* Logical operator semantics (see RFC-0002)
* Cost model details (see RFC-0007)
* Network protocols or serialization formats
* Cluster management or deployment

---

## 3. Design Principles

### 3.1 Core Principles

1. **Semantic Equivalence**
   Local and distributed execution MUST produce identical results for the same logical plan.

2. **Runtime Agnosticism**
   Physical operators and expressions are defined independently of execution runtime.

3. **Explicit Distribution**
   Data movement in distributed execution is always explicit via Exchange operators.

4. **Operator Transparency**
   Operators do not know whether they execute locally or in a distributed context.

### 3.2 Architectural Separation

The engine separates **what to compute** from **how to execute**:

| Concern | Responsibility | Layer |
|---------|----------------|-------|
| What to compute | Physical operators, expressions, schemas | Common Engine |
| How to execute locally | Pull-based pipeline, memory management | Local Runtime |
| How to execute distributed | Stage splitting, Exchange, task scheduling | Ray Runtime |

---

## 4. Crate Architecture

### 4.1 Overview

The execution engine uses a **two-crate architecture**:

- **grism-engine**: Contains both the common engine layer AND the local runtime
- **grism-ray**: Contains the distributed Ray runtime only

This design keeps the local execution path simple (no cross-crate dependencies for single-machine use) while isolating distributed complexity in a separate crate.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                   grism-engine (Common + Local Runtime)                     │
│                                                                             │
│  Common Layer:                                                              │
│  ┌─────────────┐  ┌──────────────┐  ┌───────────────┐  ┌────────────────┐   │
│  │  Physical   │  │  Operators   │  │  Expression   │  │   Physical     │   │
│  │  Plan Model │  │  (Exec)      │  │  Evaluator    │  │   Schema       │   │
│  └─────────────┘  └──────────────┘  └───────────────┘  └────────────────┘   │
│  ┌─────────────┐  ┌──────────────┐  ┌───────────────┐                       │
│  │  Operator   │  │  Schema      │  │   Memory &    │                       │
│  │  Traits     │  │  Inference   │  │   Metrics     │                       │
│  └─────────────┘  └──────────────┘  └───────────────┘                       │
│                                                                             │
│  Local Runtime:                                                             │
│  ┌────────────────────────┐  ┌────────────────────────┐                     │
│  │   LocalExecutor        │  │   LocalPhysicalPlanner │                     │
│  └────────────────────────┘  └────────────────────────┘                     │
│  ┌────────────────────────┐                                                 │
│  │   ExecutionContext     │                                                 │
│  └────────────────────────┘                                                 │
└─────────────────────────────────────────────────────────────────────────────┘
                                       │
                                       ▼
              ┌───────────────────────────────────────────────────────────────┐
              │                    grism-ray (Distributed Runtime)            │
              │  ┌─────────────────────────────────────────────────────────┐  │
              │  │   RayExecutor                                           │  │
              │  └─────────────────────────────────────────────────────────┘  │
              │  ┌─────────────────────────────────────────────────────────┐  │
              │  │   DistributedPlanner                                    │  │
              │  └─────────────────────────────────────────────────────────┘  │
              │  ┌─────────────────────────────────────────────────────────┐  │
              │  │   ExchangeExec / ExecutionStage / ArrowTransport        │  │
              │  └─────────────────────────────────────────────────────────┘  │
              └───────────────────────────────────────────────────────────────┘
```

### 4.2 Crate Responsibilities

| Crate | Responsibility | Key Types |
|-------|----------------|-----------|
| **grism-engine** | Common physical layer + local runtime | `PhysicalPlan`, `PhysicalOperator`, `ExprEvaluator`, `LocalExecutor`, `LocalPhysicalPlanner`, `ExecutionContext` |
| **grism-ray** | Distributed Ray execution | `RayExecutor`, `DistributedPlanner`, `ExchangeExec`, `ExecutionStage` |

---

## 5. Common Engine Layer (grism-engine)

The common engine layer provides all runtime-agnostic components that both local and distributed execution share.

### 5.1 Physical Plan Model

A physical plan is a tree of physical operators with associated execution properties.

```
PhysicalPlan
├── root: PhysicalOperator
└── properties: PlanProperties
    ├── execution_mode: ExecutionMode
    ├── partitioning: PartitioningSpec
    └── blocking: bool
```

**Invariants**:

* Physical plans are semantically equivalent to their source logical plans
* All expressions are executable
* All operators are compatible with the execution mode

### 5.2 Physical Operator Trait

All physical operators implement a common trait defining execution behavior:

```
PhysicalOperator
├── execute(ctx) → RecordBatchStream
├── schema() → PhysicalSchema
├── capabilities() → OperatorCaps
└── children() → [PhysicalOperator]
```

**Operator Contract**:

* Operators are stateless between executions
* Operators receive context via trait, not concrete type
* Operators produce Arrow RecordBatch streams
* Operators declare their capabilities explicitly

### 5.3 Physical Operators

The following operators are defined in the common engine layer:

| Operator | Category | Blocking | Description |
|----------|----------|----------|-------------|
| `NodeScanExec` | Source | No | Scan nodes by label |
| `HyperedgeScanExec` | Source | No | Scan hyperedges by label |
| `FilterExec` | Unary | No | Apply predicate per batch |
| `ProjectExec` | Unary | No | Compute expressions per batch |
| `LimitExec` | Unary | No | Limit output rows |
| `RenameExec` | Unary | No | Rename columns |
| `SortExec` | Blocking | **Yes** | Multi-key sorting |
| `HashAggregateExec` | Blocking | **Yes** | Aggregation with GROUP BY |
| `AdjacencyExpandExec` | Graph | No | Binary edge traversal |
| `RoleExpandExec` | Graph | No | N-ary hyperedge traversal |
| `UnionExec` | Binary | No | Union of two inputs |
| `CollectExec` | Sink | **Yes** | Collect all results |
| `EmptyExec` | Source | No | Empty result |

### 5.4 Operator Capabilities

Each operator declares its capabilities:

```
OperatorCaps
├── streaming: bool      // Can process input incrementally
├── blocking: bool       // Must consume all input before output
├── parallel_safe: bool  // Safe to execute in parallel
├── requires_partitioning: Option<PartitioningSpec>
└── scan_caps: Option<ScanCaps>  // For source operators only
```

Source operators (scans) additionally declare pushdown capabilities:

```
ScanCaps
├── predicate_pushdown: bool   // Supports predicate pushdown
├── projection_pushdown: bool  // Supports projection pushdown
├── limit_pushdown: bool       // Supports limit pushdown
└── vector_search: bool        // Supports vector similarity search
```

These capabilities inform runtime decisions:

* Local runtime uses blocking information for memory management
* Ray runtime uses capabilities for stage splitting and Exchange insertion

### 5.5 Expression Evaluator

The `ExprEvaluator` converts logical expressions to Arrow arrays:

**Supported Expression Types**:

| Category | Operations |
|----------|------------|
| Literals | Int64, Float64, String, Bool, Null |
| Column References | Direct, qualified (`entity.column`) |
| Binary Operations | `+`, `-`, `*`, `/`, `%`, `=`, `<>`, `<`, `<=`, `>`, `>=`, `AND`, `OR` |
| Unary Operations | `NOT`, `IS NULL`, `IS NOT NULL`, `NEG` |
| Special | `CASE`, `IN`, `BETWEEN` |

**Evaluation Contract**:

* Expressions are evaluated against a single RecordBatch
* Evaluation is side-effect free
* Type coercion follows RFC-0003 rules
* Null handling uses three-valued logic

### 5.6 Schema Inference

Schema inference provides type information for physical planning:

| Function | Purpose |
|----------|---------|
| `infer_expr_type()` | Infer Arrow DataType from expression |
| `build_project_schema()` | Build schema for projections |
| `build_aggregate_schema()` | Build schema for aggregations |

### 5.7 Execution Context Trait

The execution context trait abstracts runtime-specific resources:

```
ExecutionContextTrait
├── storage() → Storage
├── snapshot_id() → SnapshotId
├── memory_manager() → MemoryManager
├── metrics_sink() → Option<MetricsSink>
└── is_cancelled() → bool
```

Both local and Ray runtimes implement this trait with their specific resource management.

### 5.8 Memory and Metrics

**Memory Management**:

```
MemoryManager
├── try_reserve(bytes) → Result<Reservation>
├── current_usage() → usize
└── limit() → Option<usize>
```

**Metrics Collection**:

```
MetricsSink
├── record_rows(operator, count)
├── record_batches(operator, count)
└── record_time(operator, duration)
```

---

## 6. Local Runtime (in grism-engine)

The local runtime provides single-machine execution with pull-based streaming. It is implemented directly in `grism-engine` alongside the common engine layer, avoiding unnecessary crate boundaries for single-machine use cases.

### 6.1 Execution Model

Local execution uses a **pull-based pipeline** model:

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│    Scan     │ ──▶ │   Filter    │ ──▶ │   Project   │
│   (pull)    │     │   (pull)    │     │   (pull)    │
└─────────────┘     └─────────────┘     └─────────────┘
       ▲                   ▲                   ▲
       │                   │                   │
   RecordBatch         RecordBatch         RecordBatch
```

**Characteristics**:

* Each operator pulls from its children on demand
* Streaming execution minimizes memory usage
* Blocking operators buffer internally
* Single-threaded by default (async-ready)

### 6.2 LocalExecutor

The `LocalExecutor` drives plan execution:

```
LocalExecutor
├── execute(plan, storage, snapshot) → ExecutionResult
└── config: RuntimeConfig
    ├── batch_size: usize
    └── memory_limit: Option<usize>
```

**Execution Flow**:

1. Create execution context with storage and configuration
2. Initialize operator tree from physical plan
3. Pull batches from root operator until exhausted
4. Collect results into `ExecutionResult`

### 6.3 LocalPhysicalPlanner

The `LocalPhysicalPlanner` converts logical plans to physical plans:

```
LocalPhysicalPlanner
├── plan(logical) → PhysicalPlan
└── config: PlannerConfig
```

**Planning Rules**:

* No Exchange operators are inserted
* All operators execute in a single stage
* Operator selection based on capabilities and cost

### 6.4 LocalExecutionContext

Implements `ExecutionContextTrait` for local execution:

```
LocalExecutionContext
├── storage: Arc<dyn Storage>
├── snapshot_id: SnapshotId
├── memory_manager: Arc<dyn MemoryManager>
├── metrics_sink: Option<Arc<dyn MetricsSink>>
└── cancellation: CancellationHandle
```

### 6.5 Storage Support

The local runtime supports multiple storage backends:

| Backend | Description | Use Case |
|---------|-------------|----------|
| `InMemoryStorage` | Hash-map based storage | Testing, small datasets |
| `LanceStorage` | Lance format file storage | Production, large datasets |

Storage is accessed through the `Storage` trait defined in `grism-storage`.

---

## 7. Ray Runtime (grism-ray)

The Ray runtime provides distributed execution using Ray as the orchestration layer.

### 7.1 Execution Model

Distributed execution uses a **stage-based** model:

```
┌──────────────────────────────────────────────────────────────────────┐
│                        Distributed Plan                              │
├──────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  Stage 0 (parallel)       Exchange        Stage 1 (parallel)         │
│  ┌─────────────────┐    ┌─────────┐     ┌─────────────────┐          │
│  │ Scan → Filter   │───▶│ Shuffle │────▶│ Agg → Collect   │          │
│  │ → Project       │    │ (Hash)  │     │                 │          │
│  └─────────────────┘    └─────────┘     └─────────────────┘          │
│         │                                      │                     │
│  ┌──────┴──────┐                        ┌──────┴──────┐              │
│  │ Worker 1-N  │                        │ Worker 1-M  │              │
│  └─────────────┘                        └─────────────┘              │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**Principle**:

> **Ray orchestrates, Rust executes.**

Ray handles task scheduling, data movement, and fault tolerance, while Rust workers perform actual query execution using the same operators as local execution.

### 7.2 Exchange Operator

`ExchangeExec` is a **first-class physical operator** that:

* Repartitions data across workers
* Introduces a synchronization boundary
* Separates execution stages

```
ExchangeExec
├── partitioning: PartitioningSpec
├── mode: ExchangeMode
└── child: PhysicalOperator
```

**Exchange Modes**:

| Mode | Description |
|------|-------------|
| `Shuffle` | Repartition across workers by key |
| `Broadcast` | Replicate to all workers |
| `Gather` | Collect to single coordinator |

### 7.3 Partitioning Specification

```
PartitioningSpec
├── Hash { keys, partitions }
├── Range { key, ranges }
├── Adjacency { entity_type }
├── Single
└── RoundRobin { partitions }
```

**Partitioning Invariants**:

* Partitions MUST cover entire input
* Partitions MUST be disjoint (except Broadcast)
* Same input always maps to same partition

### 7.4 Execution Stage

A **Stage** is a connected sub-DAG of physical operators executed as a unit:

```
ExecutionStage
├── stage_id: StageId
├── plan: PhysicalSubPlan
├── input_partitioning: Option<PartitioningSpec>
└── output_partitioning: Option<PartitioningSpec>
```

**Stage Properties**:

* Contains no internal Exchange operators
* Executed as a unit on one or more workers
* Has explicit input and output partitioning

### 7.5 Stage Splitting Algorithm

Stage boundaries are determined by the following rules:

**A new stage MUST start at**:

1. Any `ExchangeExec` operator
2. Any **blocking operator** in distributed mode (as defined in RFC-0008)
3. Any operator requiring global state

**Splitting Algorithm**:

```
for node in plan.topological_order():
    if node.is_exchange() or node.is_blocking():
        stages.push(current_stage.finish())
        current_stage = new_stage()
    current_stage.add(node)
stages.push(current_stage.finish())
```

### 7.6 DistributedPlanner

The `DistributedPlanner` transforms logical plans for distributed execution:

```
DistributedPlanner
├── plan(logical) → DistributedPlan
├── split_into_stages(physical) → [ExecutionStage]
└── config: DistributedPlannerConfig
```

**Planning Responsibilities**:

* Insert Exchange operators at appropriate points
* Choose partitioning strategies based on operators
* Split plan into execution stages
* Configure stage parallelism

### 7.7 RayExecutor

The `RayExecutor` orchestrates distributed execution:

```
RayExecutor
├── execute(stages, storage) → ExecutionResult
└── config: RayExecutorConfig
    ├── default_parallelism: usize
    └── ray_address: String
```

**Execution Flow**:

1. Submit stage DAG to Ray
2. Ray schedules stage tasks on workers
3. Workers execute using Rust operators
4. Exchange moves data between stages via Arrow IPC
5. Final stage collects results

### 7.8 Data Transport

Data moves between stages via Arrow IPC:

```
Stage A Worker → Arrow IPC → Ray Object Store → Arrow IPC → Stage B Worker
```

**Transport Guarantees**:

* Zero-copy when possible via Ray Plasma
* Backpressure support
* Deterministic routing

---

## 8. Blocking Operators in Distributed Context

### 8.1 Two-Phase Aggregation

Aggregation in distributed execution uses a two-phase pattern:

```
Stage 0: PartialAggregate (parallel on each partition)
   ↓ Exchange(Hash by group key)
Stage 1: FinalAggregate (parallel by group)
```

**Aggregate Functions**:

| Function | Partial State | Merge Operation |
|----------|---------------|-----------------|
| `COUNT` | count | sum of counts |
| `SUM` | sum | sum of sums |
| `AVG` | (sum, count) | sum of sums / sum of counts |
| `MIN` | min | min of mins |
| `MAX` | max | max of maxs |

### 8.2 Two-Phase Sort

Sorting in distributed execution:

```
Stage 0: LocalSort (parallel per partition)
   ↓ Exchange(Range)
Stage 1: MergeSort (parallel per range)
```

---

## 9. Graph Operators in Distributed Context

### 9.1 Adjacency-Aware Partitioning

For graph traversal workloads, adjacency-based partitioning keeps most expansions local:

```
PartitioningSpec::Adjacency { entity: Node }
```

**Benefits**:

* Most binary Expand operations are partition-local
* Reduces shuffle volume for traversal queries
* Preserves locality for multi-hop patterns

**Note**: Adjacency partitioning is orthogonal to adjacency access paths (RFC-0009); it does not imply the presence of adjacency indexes.

### 9.2 Expand Distribution

| Expand Type | Distribution Strategy |
|-------------|----------------------|
| Binary (AdjacencyExpand) | Prefer adjacency partitioning |
| N-ary (RoleExpand) | May require Exchange for non-local roles |

**Cross-Partition Expand**:

When expansion crosses partition boundaries, explicit Exchange is inserted:

```
Stage 0: Scan nodes (partitioned by node_id)
   ↓ Exchange(Adjacency)
Stage 1: RoleExpand (hyperedges co-located with participating nodes)
```

---

## 10. Failure and Recovery

### 10.1 Failure Domains

Stage boundaries define **failure domains**:

| Failure | Recovery Action |
|---------|-----------------|
| Worker crash | Retry stage on different worker |
| Exchange failure | Re-run upstream stage |
| Coordinator crash | Query fails, client retries |

### 10.2 Retry Semantics

* Stateless operators may be safely retried
* Partial stage results are discarded on failure
* Exactly-once semantics are **not guaranteed** by default

---

## 11. Explainability

### 11.1 EXPLAIN Output

Both runtimes support `EXPLAIN` output:

**Local EXPLAIN**:

```
NodeScan(Person)
  └── Filter(age > 21)
        └── Project(name, age)
              └── Collect
```

**Distributed EXPLAIN**:

```
Stage 0 (parallelism=8):
  NodeScan(Person) → Filter(age > 21) → Project(name, age)
  Output: HashPartition(node_id)

    ↓ Exchange(Shuffle, Hash(node_id))

Stage 1 (parallelism=4):
  Aggregate(COUNT(*), GROUP BY city)
  Output: HashPartition(city)

    ↓ Exchange(Gather)

Stage 2 (parallelism=1):
  Collect
```

### 11.2 Runtime Metrics

Both runtimes collect per-operator metrics:

| Metric | Description |
|--------|-------------|
| `rows_in` | Input row count |
| `rows_out` | Output row count |
| `batches` | Number of batches processed |
| `execution_time` | Wall-clock execution time |
| `memory_peak` | Peak memory usage |

---

## 12. API Compatibility

### 12.1 Local Execution API

```python
from grism import LocalExecutor, LocalPhysicalPlanner

planner = LocalPhysicalPlanner()
plan = planner.plan(logical_plan)

executor = LocalExecutor()
result = executor.execute(plan, storage, snapshot)

for batch in result.batches:
    process(batch)
```

### 12.2 Distributed Execution API

```python
from grism import RayExecutor, DistributedPlanner

planner = DistributedPlanner()
stages = planner.plan(logical_plan)

executor = RayExecutor.connect("ray://cluster:10001")
result = executor.execute(stages, storage)

for batch in result.batches:
    process(batch)
```

### 12.3 Unified Interface

A unified interface can select runtime based on configuration:

```python
from grism import Hypergraph

hg = Hypergraph.connect("grism://local")  # Local execution
# or
hg = Hypergraph.connect("grism://ray:10001")  # Ray execution

# Same query API regardless of runtime
result = (
    hg.nodes("Person")
      .filter(col("age") > 21)
      .select("name", "age")
      .collect()
)
```

---

## 13. Relationship to Other RFCs

| RFC | Relationship |
|-----|--------------|
| **RFC-0002** | Defines logical operator semantics that physical operators implement |
| **RFC-0003** | Defines expression semantics that ExprEvaluator implements |
| **RFC-0008** | Defines abstract physical operator contracts (this RFC implements them) |
| **RFC-0010** | Defines distributed execution semantics (grism-ray implements them) |
| **RFC-0012** | Defines storage contracts that both runtimes use |
| **RFC-0100** | Defines overall architecture that this RFC refines for execution |

**Authoritative Reference**: This RFC (RFC-0102) is the authoritative implementation reference for execution engine architecture. RFC-0008 defines *what operators must implement* (the abstract contract), while this RFC defines *how they are actually built*. Similarly, RFC-0010 defines *what distribution must preserve* (semantic constraints), while this RFC defines *how Ray runtime achieves it*.

---

## 14. Guarantees

This RFC guarantees:

1. **Semantic Equivalence**: Local and Ray execution produce identical results
2. **Operator Transparency**: Operators are unaware of their execution context
3. **Explicit Distribution**: All data movement is visible in distributed plans
4. **Hypergraph Correctness**: Expand operators preserve adjacency semantics
5. **Explainability**: Both runtimes support plan explanation and metrics

---

## 15. Open Questions

* Adaptive repartitioning based on runtime statistics
* Speculative execution for stragglers
* Hybrid local/distributed execution for mixed workloads
* GPU operator acceleration
* Spill-to-disk for memory-constrained execution
* Distributed approximate operators (e.g., vector search) may violate global top-K guarantees unless explicitly merged

---

## 16. Conclusion

This RFC defines the execution engine architecture for Grism, establishing a clean separation between runtime-agnostic components and runtime-specific implementations.

> **The engine defines what to compute.**
> **The runtime defines how to execute.**
> **RFC-0102 defines their boundary.**
