# RFC-0014: Multi-Modal Data Processing

**Status**: Draft
**Authors**: Grism Team
**Created**: 2026-01-21
**Last Updated**: 2026-01-21
**Depends on**: RFC-0003, RFC-0008, RFC-0012, RFC-0013
**Supersedes**: —

---

## 1. Abstract

This RFC defines the **multi-modal data processing model** for Hypergraph.

Leveraging **Lance’s AI-native, columnar design**, Hypergraph supports images, video, audio, text, and other modalities as **queryable, indexable, and semantically interpretable data**, not opaque payloads.

This RFC specifies:

* Multi-modal data types and storage contracts
* Modality-aware expressions and operators
* Feature extraction and embedding integration
* Execution and reasoning boundaries

Multi-modal processing is **explicit, inspectable, and reproducible**.

---

## 2. Scope and Non-Goals

### 2.1 Scope

This RFC specifies:

* Multi-modal logical and physical data types
* Storage and access semantics for rich media
* Feature extraction as first-class computation
* Vector and temporal access paths
* Integration with semantic reasoning (RFC-0013)

### 2.2 Non-Goals

This RFC does **not** define:

* Model training or fine-tuning pipelines
* Media encoding standards
* UI or visualization concerns
* Streaming ingestion protocols
* End-user annotation workflows

---

## 3. Design Principles

1. **Modalities Are Data, Not Blobs**
   Images and video are queryable structures.

2. **Lance-Native by Construction**
   Storage design aligns with Lance’s columnar + vector layout.

3. **Explicit Feature Semantics**
   Features and embeddings never appear implicitly.

4. **Temporal Is Structural**
   Time is part of the data model, not metadata.

---

## 4. Multi-Modal Data Model

### 4.1 Modal Entities

Hypergraph supports the following **modal entities**:

| Modality | Logical Type |
| -------- | ------------ |
| Text     | `Text`       |
| Image    | `Image`      |
| Video    | `Video`      |
| Audio    | `Audio`      |
| Document | `Document`   |
| Frame    | `Frame`      |
| Segment  | `Segment`    |

Each modal entity is a **structured value**, not raw bytes.

---

### 4.2 Modal Properties

Modal entities MAY expose:

* Dimensions (height, width, duration)
* Encoding metadata
* Temporal offsets
* Channel information

These properties are **queryable expressions** (RFC-0003).

---

## 5. Storage Semantics (Lance-Aligned)

### 5.1 Columnar Modal Storage

Multi-modal data MUST be stored in **Lance columnar form**:

* Media payloads stored as Lance binary blobs or chunked Arrow arrays
* Metadata stored in aligned Arrow columns
* Temporal slicing supported natively via fragment metadata

Rules:

* No row-local blob packing (Lance handles this)
* Zero-copy reads via Arrow preferred
* Partial reads MUST be supported via fragment scanning

---

### 5.2 Chunking & Tiling

For large media (video, long audio):

* Data MUST be chunked
* Chunks MUST align with semantic boundaries (frames, segments)
* Chunk metadata MUST be exposed

Chunking is **observable**, not hidden.

---

## 6. Modal Expressions & Operators

### 6.1 Modal Expressions

New expression categories extend RFC-0003:

* `Decode(media)`
* `FrameAt(video, timestamp)`
* `Slice(media, range)`
* `Duration(media)`
* `Resolution(image)`

Modal expressions are:

* Deterministic
* Side-effect free
* Explicitly typed

---

### 6.2 Feature Extraction Operators

Feature extraction is an explicit operator:

```
ExtractFeature {
  input: Modal
  model: FeatureModel
  output: Embedding | Tensor
}
```

Rules:

* Feature extraction NEVER happens implicitly
* Model identity and version MUST be recorded
* Outputs are first-class columns

---

## 7. Vector & Temporal Access Paths

### 7.1 Vector Indexes for Modal Data

Modal features MAY be indexed via vector indexes (RFC-0009).

Rules:

* Index applies to extracted features, not raw media
* Approximation MUST be declared
* Index validity tied to feature version

---

### 7.2 Temporal Access Paths

Video and audio support temporal access paths:

* Frame-based scans
* Segment-based scans
* Time-range predicates

Temporal access paths MUST preserve ordering semantics when requested.

---

## 8. Execution Semantics

### 8.1 Modal Operators in Physical Plans

Modal operators participate fully in RFC-0008:

* Batch-oriented
* Backpressure-aware (RFC-0011)
* Parallelizable across chunks

Media decoding MAY be:

* Lazy
* Parallel
* GPU-accelerated (optional)

---

### 8.2 Cost Model Interaction

Cost factors include:

* Decode cost
* Feature extraction cost
* Chunk fan-out
* Vector search complexity

Missing statistics MUST bias toward conservative execution (RFC-0007).

---

## 9. Distributed & Parallel Processing

### 9.1 Data Parallelism

Multi-modal execution supports:

* Frame-level parallelism
* Segment-level parallelism
* Feature-level parallelism

Chunks are the unit of distribution.

---

### 9.2 Locality Constraints

* Feature extraction prefers data locality
* Video decoding SHOULD be co-located with storage
* Cross-node shuffles are explicit

---

## 10. Semantic Reasoning over Modal Data

### 10.1 Modal Assertions

Reasoning MAY assert:

```
(Frame, depicts, Object)
(Segment, contains, Event)
(Video, about, Concept)
```

Assertions MUST include provenance (RFC-0013).

---

### 10.2 Neurosymbolic Guards

Neural outputs (e.g. vision models):

* Provide scores or candidates
* MUST be filtered by symbolic rules
* MUST NOT directly assert facts

This prevents “model hallucination → graph truth”.

---

## 11. Materialization & Persistence

### 11.1 Derived Modal Data

Derived artifacts MAY include:

* Extracted frames
* Feature embeddings
* Annotated segments

Rules:

* Derived data is versioned
* Base media is immutable
* Rebuildability required

---

### 11.2 Storage Guarantees

Storage MUST guarantee:

* Referential integrity between media and features
* Snapshot consistency across derived data
* Deterministic rebuilds

---

## 12. Explainability & Auditing

EXPLAIN MUST show:

* Modal operators involved
* Feature models used
* Index usage
* Decode vs compute costs

Every derived semantic fact MUST be explainable.

---

## 13. Error Handling

Modal-specific errors:

| Error               | Meaning            |
| ------------------- | ------------------ |
| DecodeFailure       | Media unreadable   |
| FeatureMismatch     | Model incompatible |
| TemporalOutOfBounds | Invalid slice      |
| ModalTypeError      | Wrong modality     |

Errors MUST be explicit and non-silent.

---

## 14. Relationship to Other RFCs

* **RFC-0003**: Modal expressions & types
* **RFC-0008**: Modal physical operators
* **RFC-0009**: Vector & temporal indexes
* **RFC-0012**: Lance-aligned storage
* **RFC-0013**: Semantic reasoning over modal facts

RFC-0014 defines **how perception enters the system**.

---

## 15. Open Questions

* Streaming video reasoning
* Cross-modal joins (text ↔ video ↔ graph)
* Learned chunking strategies
* GPU scheduling semantics

---

## 16. Conclusion

This RFC defines **multi-modal cognition** in Hypergraph.

> **Tables store facts.
> Graphs store relationships.
> Multi-modal data stores perception.
> RFC-0014 makes perception computable through Lance and Arrow.**
